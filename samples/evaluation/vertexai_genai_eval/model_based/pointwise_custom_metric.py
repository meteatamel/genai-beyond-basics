import pandas
import sys
from vertexai.evaluation import (
    constants,
    EvalTask,
    PointwiseMetric,
    PointwiseMetricPromptTemplate,
    MetricPromptTemplateExamples
)

sys.path.append("../../../../")
from samples.evaluation.vertexai_genai_eval.utils import get_experiment_name, print_eval_result

# Pointwise with a custom metric
# https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval#model-based-metrics

responses = [
    "This is a funny and single sentence",
    "This is a funny sentence. But it's 2 sentences",
    "This is neither funny. Nor a single sentence"
]

eval_dataset = pandas.DataFrame(
    {
        "response": responses,
    }
)

def template():

    # Define a pointwise metric with two criteria
    custom_metric_prompt_template = PointwiseMetricPromptTemplate(
            criteria={
                "one_sentence": (
                    "The response is one short sentence."
                ),
                "entertaining": (
                    "The response is entertaining."
                ),
            },
            rating_rubric={
                "3": "The response performs well on both criteria.",
                "2": "The response performs well with only one of the criteria.",
                "1": "The response falls short on both criteria",
            },
        )

    print(f"custom_metric_prompt_template: {custom_metric_prompt_template}")

    custom_metric = PointwiseMetric(
        metric="custom_metric",
        metric_prompt_template=custom_metric_prompt_template
    )

    eval_task = EvalTask(
        dataset=eval_dataset,
        metrics=[custom_metric],
        experiment=get_experiment_name(__file__, "template")
    )

    eval_result = eval_task.evaluate()
    print_eval_result(eval_result, colwidth=100)


def free_form():

    # You can edit an existing prompt template
    fluency_prompt_template = MetricPromptTemplateExamples.get_prompt_template(constants.Metric.FLUENCY)
    print(f"fluency_prompt_template: {fluency_prompt_template}")

    # Or create your own prompt template
    custom_metric_prompt_template = """
    # Instruction
    You are an expert evaluator. Your task is to evaluate the quality of the responses generated by AI models. We will provide you with the user prompt and an AI-generated responses.
    You should first read the user input carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.
    You will assign the response a rating following the Rating Rubric and Evaluation Steps. Give step by step explanations for your rating, and only choose ratings from the Rating Rubric.
    
    # Evaluation
    ## Criteria
    entertaining: The response is entertaining.
    one_sentence: The response is one short sentence.
    
    ## Rating Rubric
    1: The response falls short on both criteria
    2: The response performs well with only one of the criteria.
    3: The response performs well on both criteria.
    
    ## Evaluation Steps
    Step 1: Assess the response in aspects of all criteria provided. Provide assessment according to each criterion.
    Step 2: Score based on the rating rubric. Give a brief rationale to explain your evaluation considering each individual criterion.
    
    # User Inputs and AI-generated Response
    ## User Inputs
    
    ## AI-generated Response
    {response}
    """
    print(f"custom_metric_prompt_template: {custom_metric_prompt_template}")

    custom_metric = PointwiseMetric(
        metric="custom_metric",
        metric_prompt_template=custom_metric_prompt_template
    )

    eval_task = EvalTask(
        dataset=eval_dataset,
        metrics=[custom_metric],
        experiment=get_experiment_name(__file__, "free-form")
    )

    eval_result = eval_task.evaluate()
    print_eval_result(eval_result, colwidth=100)


if __name__ == "__main__":
    globals()[sys.argv[1]]()
